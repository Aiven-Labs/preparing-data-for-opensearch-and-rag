{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing your data for the model\n",
    "\n",
    "Now that you have your data, you need to prepare it for use with our chosen model.\n",
    "\n",
    "We're using the [all-mpnet-base-v2 from Huggingface](https://huggingface.co/sentence-transformers/all-mpnet-base-v2). This is a very common model used for natural language processing and similarity search.\n",
    "\n",
    "To make our content usable with this model, we need to segment our code into chunks.\n",
    "\n",
    "Models will often have a character or a token limit. `allmpnet-base-v2` has a limit of 384 characters, truncating any characters more than that.\n",
    "\n",
    "We want to make sure that we get AS COMPLETE of a thought as possible. That is to say a complete thought split into two segments will likely both be detected vs having a truncated thought that may include irrelevant content.\n",
    "\n",
    "We're going to split our content into tweet-like segments of approximately 300 characters with a buffer around 20 charachters. We'll also split on phrase boundaries like punctuation or newlines.\n",
    "\n",
    "**This is a choice**. You can experiment with the parameters to fit your needs. You can also look at other models that have "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to read the contents of our text file.\n",
    "\n",
    "LangChain is a great way to wrap around the work that we're doing in this.\n",
    "\n",
    "LangChain gives us the ability to select our [embeddings](https://python.langchain.com/docs/integrations/text_embedding/). It also gives us an interface to perform a [recursive split by character](https://python.langchain.com/docs/modules/data_connection/document_transformers/recursive_text_splitter/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's Make it Happen\n",
    "\n",
    "Let's create a function that:\n",
    "- reads the file\n",
    "- parses the metadata from the file\n",
    "- splits the content into separate documents with unique identifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import frontmatter\n",
    "import os\n",
    "import pathlib\n",
    "import uuid\n",
    "\n",
    "import arrow\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from opensearchpy import OpenSearch, helpers\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "INDEX_NAME = os.getenv(\"INDEX_NAME\")\n",
    "CONNECTION_STRING = os.getenv(\"OPENSEARCH_SERVICE_URI\")\n",
    "client = OpenSearch(connection_string, use_ssl=True, timeout=100)\n",
    "\n",
    "fmt = r\"MMMM[\\s+]D[\\w+,\\s+]YYYY\"\n",
    "\n",
    "# define splitter    \n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=300,\n",
    "    chunk_overlap=20,\n",
    "    separators=[\".\", \"!\", \"?\", \"\\n\"],\n",
    ")\n",
    "\n",
    "# define embeddings. These options are all the defaults and not explicitly needed.\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name = \"sentence-transformers/all-mpnet-base-v2\",\n",
    "    model_kwargs = {'device': 'cpu'},\n",
    "    encode_kwargs = {'normalize_embeddings': False}\n",
    ")\n",
    "\n",
    "    \n",
    "def load_data(file: pathlib.Path):\n",
    "    \"\"\"Chunk data, create embeddings, and index in OpenSearch.\"\"\"\n",
    "    frontmatter_post = frontmatter.loads(file.read_text()) # loads the metadata from the file\n",
    "    base_data = {\n",
    "            \"_index\": INDEX_NAME,\n",
    "            \"title\": frontmatter_post[\"title\"],\n",
    "            \"description\": frontmatter_post[\"description\"],\n",
    "            \"url\": frontmatter_post[\"url\"],\n",
    "            \"pub_date\": arrow.get(frontmatter_post[\"pub_date\"], fmt).date().isoformat(),\n",
    "        }\n",
    "    \n",
    "    docs = []\n",
    "    \n",
    "    post_chunks = splitter.create_documents([frontmatter_post.content])\n",
    "    for post_chunk in post_chunks:\n",
    "        doc = {\n",
    "            **base_data, \n",
    "            **{\n",
    "                \"_id\": str(uuid.uuid4()),\n",
    "                \"content\": post_chunk.page_content,\n",
    "                \"content_vector\": embeddings.embed_documents([post_chunk.page_content])[0]\n",
    "                }\n",
    "            }\n",
    "        docs.append(doc)\n",
    "    print(len(docs))        \n",
    "    response = helpers.bulk(client, docs)\n",
    "    return response\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our function, we'll pass that function into our opensearch bulk function. This will allow us to ingest the documents one at a time, making it easier to restart in the event of an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "233\n",
      "155\n",
      "184\n",
      "179\n",
      "151\n",
      "171\n",
      "167\n",
      "181\n",
      "180\n",
      "223\n",
      "35\n",
      "197\n",
      "169\n",
      "186\n",
      "163\n",
      "125\n",
      "190\n",
      "152\n",
      "142\n",
      "203\n",
      "205\n",
      "206\n",
      "152\n",
      "157\n",
      "141\n",
      "246\n",
      "290\n",
      "221\n",
      "188\n",
      "167\n",
      "151\n",
      "186\n",
      "258\n",
      "218\n",
      "195\n",
      "157\n",
      "151\n",
      "214\n",
      "225\n",
      "207\n",
      "131\n",
      "216\n",
      "199\n",
      "147\n",
      "181\n",
      "203\n",
      "148\n",
      "210\n",
      "150\n",
      "212\n",
      "154\n",
      "151\n",
      "294\n",
      "198\n",
      "308\n",
      "184\n",
      "151\n",
      "168\n",
      "153\n",
      "232\n",
      "160\n",
      "233\n",
      "127\n",
      "189\n",
      "188\n",
      "136\n",
      "139\n",
      "178\n",
      "184\n",
      "201\n",
      "169\n",
      "138\n",
      "186\n"
     ]
    }
   ],
   "source": [
    "directory = pathlib.Path(\"../transcripts\")\n",
    "\n",
    "for file in directory.iterdir():\n",
    "    load_data(file)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
