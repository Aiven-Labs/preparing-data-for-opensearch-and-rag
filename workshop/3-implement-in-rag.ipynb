{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing our RAG Application\n",
    "\n",
    "We've walked through the process of ingesting our data. Now we want to setup our search application.\n",
    "\n",
    "There are multiple ways of implementing search that we won't be addressing in this workshop.\n",
    "\n",
    "We're going to implement a similarity search. We can choose to use the OpenSearch®️ SDK or we can use LangChain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity Search with Langchain\n",
    "\n",
    "LangChain allows us to connect our existing configurations across our platform to ensure that we settings are consistent. This is paramount to the success of your similarity search in that if you're search parameters differ from the values you selected to load data from, you can run into some issues.\n",
    "\n",
    "LangChain gives us the ability to access a [preexisting OpenSearch instance](https://python.langchain.com/docs/integrations/vectorstores/opensearch/#using-a-preexisting-opensearch-instance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pprint import pprint\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import OpenSearchVectorSearch\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings()\n",
    "vector_search = OpenSearchVectorSearch(\n",
    "    index_name=os.getenv(\"INDEX_NAME\"),\n",
    "    embedding_function=embeddings,\n",
    "    opensearch_url=os.getenv(\"OPENSEARCH_SERVICE_URI\"),\n",
    ")\n",
    "\n",
    "query = \"How can I be productive\"\n",
    "\n",
    "results = vector_search.similarity_search(\n",
    "    query,\n",
    "    vector_field=\"content_vector\",\n",
    "    text_field=\"content\",\n",
    "    metadata_field=\"*\",\n",
    ")\n",
    "\n",
    "pprint(results[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "llm = ChatOllama(model=\"llama3\")\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "        Offer supportive advice for the question {query} with supporting quotes from \n",
    "        \"{docs}\".\n",
    "                                          \n",
    "        Don't include quotes from other sources.\n",
    "\"\"\")\n",
    "\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "print(chain.invoke({\"query\": query, \"docs\": \"\\n\".join([result.page_content for result in results])}))\n",
    "print(\"-------------------\")\n",
    "print(f'Here are some episodes that might help you with \"{query}\":')\n",
    "episodes = set()\n",
    "for result in results:\n",
    "    episodes.add(f\"{result.metadata[\"title\"]} - {result.metadata[\"url\"]}\")\n",
    "print(\"\\n\".join(episodes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is good but we probably don't want to use Ollama in our production environment. Let look at how easy it is to use a different model.\n",
    "\n",
    "> NOTE: ⚠️ The next block uses the OpenAI API which cannot be used without an API key which you will need to pay for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\n",
    "     \"\"\"Offer supportive advice for the question {query} with supporting quotes from \n",
    "     ---\n",
    "     {docs}\n",
    "     ---\n",
    "     Wrap quotes in quotation marks. Don't include quotes from other sources.\n",
    "     If there are no documents to quote, say \\\"I don't have any information on that.\\\"\"\"\")\n",
    "    ,\n",
    "    (\"user\",\n",
    "     \"{query}\"),\n",
    "])\n",
    "\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "print(chain.invoke({\"query\": query, \"docs\": \"\\n\".join([result.page_content for result in results])}))\n",
    "print(\"-------------------\")\n",
    "print(f'Here are some episodes that might help you with \"{query}\":')\n",
    "episodes = set()\n",
    "for result in results:\n",
    "    episodes.add(f\"{result.metadata[\"title\"]} - {result.metadata[\"url\"]}\")\n",
    "print(\"\\n\".join(episodes))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
